Total time: 10.8084 s
File: /home/jianghan/Devel/workspace/src/MPPI/manipulator_mppi/control/controllers/mppi_manipulation.py
Function: update at line 96

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    96                                               @profile
    97                                               def update(self, obs):
    98                                                   """
    99                                                   Update the MPPI controller based on the current observation.
   100                                           
   101                                                   Args:
   102                                                       obs (np.ndarray): Current state observation.
   103                                                   Returns:
   104                                                       np.ndarray: Selected action based on the optimal trajectory.
   105                                                   """
   106                                                    # Generate perturbed actions for rollouts
   107       499     786633.2   1576.4      7.3          actions = self.perturb_action()
   108       499        627.2      1.3      0.0          self.obs = obs
   109                                           
   110                                                   # Perform rollouts using threaded rollout function
   111       499    8451381.9  16936.6     78.2          self.rollout_func(self.rollout_models, self.state_rollouts, actions, np.repeat(np.array([np.concatenate([[0], obs])]), self.n_samples, axis=0), self.sensor_datas, num_workers=self.num_workers, nstep=self.horizon)
   112                                                   
   113                                                   # self.rollout_func(self.state_rollouts, actions, np.repeat(
   114                                                   #     np.array([np.concatenate([[0], obs])]), self.n_samples, axis=0), self.sensor_datas,
   115                                                   #     num_workers=self.num_workers, nstep=self.horizon)
   116                                           
   117                                           
   118                                                   # Calculate costs for each sampled trajectory
   119       499    1440893.0   2887.6     13.3          costs_sum = self.cost_func(self.state_rollouts[:, :, 1:], actions, self.sensor_datas, self.joints_ref, self.tips_frame_pos_ref, self.cube_state_ref)
   120                                           
   121                                                   # Calculate MPPI weights for the samples
   122       499      11340.0     22.7      0.1          min_cost = np.min(costs_sum)
   123       499       9832.0     19.7      0.1          max_cost = np.max(costs_sum)
   124       499       7495.7     15.0      0.1          self.exp_weights = np.exp(-1 / self.temperature * ((costs_sum - min_cost) / (max_cost - min_cost)))
   125                                           
   126                                                   # Weighted average of action deltas
   127       499      17791.3     35.7      0.2          weighted_delta_u = self.exp_weights.reshape(self.n_samples, 1, 1) * actions
   128       499      42038.7     84.2      0.4          weighted_delta_u = np.sum(weighted_delta_u, axis=0) / (np.sum(self.exp_weights) + 1e-10)
   129       499      11699.2     23.4      0.1          updated_actions = np.clip(weighted_delta_u, self.act_min, self.act_max)
   130                                           
   131                                                   # Update the trajectory with the optimal action
   132       499        230.7      0.5      0.0          self.selected_trajectory = updated_actions
   133       499      25685.7     51.5      0.2          self.trajectory = np.roll(updated_actions, shift=-1, axis=0)
   134       499       1017.8      2.0      0.0          self.trajectory[-1] = updated_actions[-1]
   135                                           
   136                                                   # Return the first action in the trajectory as the output action
   137       499       1710.6      3.4      0.0          return updated_actions[0]
